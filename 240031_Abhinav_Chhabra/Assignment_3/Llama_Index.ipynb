{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0cca47d",
   "metadata": {},
   "source": [
    "This is a notebook detailing on how to use Llama index to build agent LLM applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb6b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv('COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa252ead",
   "metadata": {},
   "source": [
    "Setting the API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b1ed89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "William Shakespeare is considered one of the greatest playwrights in English literature and is often called England's national poet. The Bard of Avon's legacy continues to influence literature and performance to this day.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\",api_key = api_key)\n",
    "response = llm.complete(\"William Shakespeare is \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58d54cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Explain about LLMs\n",
      "A: Large Language Models (LLMs) are a type of artificial intelligence (AI) that has been trained on massive amounts of text data from the internet, books, and other sources. They use deep learning techniques to generate and understand language at a level that is comparable to, or even surpasses, that of humans in some tasks. \n",
      "\n",
      "At their core, LLMs consist of neural networks with billions or even trillions of parameters. These parameters are adjusted during training as the model learns patterns and relationships in the data. The training process involves exposing the LLM to a diverse range of texts, such as books, articles, social media posts, and even code, to teach it the intricacies of language, including grammar, syntax, semantics, and context. \n",
      "\n",
      "One of the key advantages of LLMs is their ability to generate human-like responses or text based on a prompt. They can continue a story, answer questions, generate emails, and even create code, depending on their training and fine-tuning. This capability has led to a wide range of applications, including language translation, text completion, content generation, question-answering, summarization, and more. \n",
      "\n",
      "Some popular examples of LLMs include: \n",
      "1. GPT-3 (Generative Pre-trained Transformer 3) by OpenAI: This LLM has garnered significant attention for its impressive text-generation capabilities and has been used in various applications. \n",
      "2. BERT (Bidirectional Encoder Representations from Transformers): Developed by Google, BERT is commonly used for natural language processing tasks like question answering and sentiment analysis. \n",
      "3. T5 (Text-to-Text Transfer Transformer): Created by Google AI, T5 frames all language tasks as a text-to-text problem, demonstrating versatility and strong performance across various applications. \n",
      "4. RoBERTa (Robustly Optimized BERT Approach): RoBERTa is a variant of BERT that uses more data and improved training techniques to achieve state-of-the-art performance on several language understanding tasks. \n",
      "\n",
      "While LLMs have shown remarkable capabilities, they also have certain limitations and challenges. For example: \n",
      "- Hallucinations: LLMs sometimes generate factually inaccurate or inconsistent responses, known as hallucinations, which can be a significant issue in certain applications. \n",
      "- Bias and Toxicity: Training data can contain biases and toxic language, which the LLM may inadvertently pick up and reflect in its responses. \n",
      "- Lack of Common Sense Reasoning: Despite their impressive capabilities, LLMs often lack true understanding and common sense reasoning, which limits their ability to interpret context and generate appropriate responses in certain situations. \n",
      "- Energy Consumption and Environmental Impact: Training large LLMs requires substantial computational power and energy, leading to concerns about their environmental impact. \n",
      "\n",
      "Despite these challenges, LLMs represent a significant milestone in the field of artificial intelligence and natural language processing. They have the potential to revolutionize numerous industries, from customer service and content creation to software development and language translation. As research and development in this field continue to advance, we can expect to see even more sophisticated and capable LLMs in the future.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"Explain about LLMs\"),\n",
    "]\n",
    "question = messages[-1].content\n",
    "chat_response = llm.chat(messages)\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {chat_response.message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6527d2ae",
   "metadata": {},
   "source": [
    "Creating the tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cfe4106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
    "    print(\"Multiplying\")\n",
    "    return a * b\n",
    "\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0fc641e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model=\"models/gemini-2.0-flash\", \n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a1e522",
   "metadata": {},
   "source": [
    "Now we initialize a agent using the prebuilt function of Llama index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "39919557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "tools = [\n",
    "    FunctionTool.from_defaults(fn=multiply),\n",
    "    FunctionTool.from_defaults(fn=add)\n",
    "]\n",
    "workflow = FunctionCallingAgent.from_tools(\n",
    "    tools,\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are an agent that performs basic mathematical operations only using the tools provided.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55658fb7",
   "metadata": {},
   "source": [
    "Now we send in prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d7646749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiplying\n",
      "20+(2*4) is 28.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = workflow.chat(\"What is 20+(2*4)?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe950a",
   "metadata": {},
   "source": [
    "Using pre existing tools from llama hub : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bcb3172c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current stock price of NVIDIA (NVDA) is $144.12. (tools: stock_basic_info)\n",
      "\n",
      "Predicting stock growth is inherently speculative and depends on numerous factors that are impossible to foresee with certainty. Here's a blend of optimistic and cautious considerations for the next 2 years (Model):\n",
      "\n",
      "*   **AI and Data Center Growth:** NVIDIA is a major player in AI, data centers, and high-performance computing. Continued expansion in these areas could drive significant revenue growth.\n",
      "*   **Gaming Market:** The gaming market's cyclical nature could present fluctuations. However, new GPU technologies and cloud gaming initiatives could offset potential downturns.\n",
      "*   **Automotive and Other Emerging Markets:** NVIDIA's automotive partnerships and its forays into robotics and other areas could create new revenue streams.\n",
      "*   **Competition:** Competition in the GPU and AI chip markets is intense. Competitor advancements could impact NVIDIA's market share and profitability.\n",
      "*   **Economic Factors:** Broader economic conditions (recessions, inflation, interest rates) could impact overall tech spending and investor sentiment.\n",
      "*   **Valuation:** NVIDIA's valuation is high. This makes the stock sensitive to any negative news or earnings disappointments.\n",
      "\n",
      "**In summary:** A reasonable outlook would be that NVIDIA will continue to grow. However, given the current valuation, it's important to acknowledge the risk of potential corrections if growth slows or market sentiment shifts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.tools.yahoo_finance import YahooFinanceToolSpec\n",
    "\n",
    "finance_tools = YahooFinanceToolSpec().to_tool_list()\n",
    "finance_tools.extend([multiply, add]) # adding our custom tools multiply and add\n",
    "workflow = FunctionAgent(\n",
    "    name=\"Agent\",\n",
    "    description=\"Useful for performing financial operations.\",\n",
    "    llm=GoogleGenAI(model=\"models/gemini-2.0-flash\", api_key=api_key,temperature = 2),\n",
    "    tools=finance_tools,\n",
    "    system_prompt=\"You are a helpful assistant and if you cannot use tools allow the LLM model to use its creativity. Please mark all responses(by specifiying tools : ) given as generated by tools or model as well \",\n",
    ")\n",
    "response = await workflow.run(\n",
    "    user_msg=\"What's the current stock price of NVIDIA and predict its growth over the next 2 years?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12c432",
   "metadata": {},
   "source": [
    "Adding context to the workflows : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "14cb9e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Abhinav! It's nice to meet you. How can I help you today?\n",
      "\n",
      "Okay, I can help with that! Please give me a moment to access the latest weather information for Bengaluru, India.\n",
      "\n",
      "Okay, here's what I'm seeing for Bengaluru, India as of right now (this is a real-time estimate, so it's subject to change):\n",
      "\n",
      "*   **Temperature:** The current temperature is around 27Â°C.\n",
      "*   **Condition:** Sunny\n",
      "*   **Wind:** Calm Winds\n",
      "\n",
      "I hope you have a good day Abhinav.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "workflow = FunctionAgent(name = 'Contextual bot', llm = GoogleGenAI(model=\"models/gemini-2.0-flash\", api_key=api_key,temperature = 2),system_prompt = \"You are an assistant answer queries as provided\")\n",
    "ctx = Context(workflow)\n",
    "response = await workflow.run(user_msg=\"Hi, my name is Abhinav!\", ctx=ctx)\n",
    "print(response)\n",
    "response = await workflow.run(user_msg= \"Can you provide me with today's weather in Bengaluru,India\",ctx = ctx)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cecf37",
   "metadata": {},
   "source": [
    "Now we will learn how to create a streaming output in cases when output is extremely long for this we will be using Tavily tool which takes some time to execute. Tavily tool is like a retriever in a RAG instead of providing it with a dataset Tavily extracts information from the internet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bff2094b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of May 2025, according to the Bloomberg Billionaires Index, Elon Musk's net worth is estimated to be US$381 billion. Forbes estimates his net worth to be US$424.7 billion."
     ]
    }
   ],
   "source": [
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "import os\n",
    "load_dotenv()\n",
    "tavily_tool = TavilyToolSpec(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "llm = GoogleGenAI(model=\"models/gemini-2.0-flash\", api_key=api_key,temperature = 2)\n",
    "workflow = FunctionAgent(\n",
    "    tools=tavily_tool.to_tool_list(),\n",
    "    llm=llm,\n",
    "    system_prompt=\"You're a helpful assistant that can search the web for information.\",\n",
    ")\n",
    "from llama_index.core.agent.workflow import AgentStream\n",
    "handler = workflow.run(user_msg=\"What's the networth of Elon Musk currently?\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, AgentStream):\n",
    "        print(event.delta, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc198b",
   "metadata": {},
   "source": [
    "Now we will create workflows with multiple agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d5340c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Okay, I have gathered some information on the history of the World Wide Web. Here are my notes:\n",
      "\n",
      "**History of the World Wide Web**\n",
      "\n",
      "*   **Inventor:** Tim Berners-Lee\n",
      "*   **Inception:** March 12, 1989\n",
      "*   **Location:** CERN\n",
      "*   Berners-Lee published a summary of the WWW project on August 6, 1991, inviting collaborators.\n",
      "*   The Web is a service that operates over the Internet.\n",
      "*   The development of the World Wide Web was begun in 1989 by Tim Berners-Lee and his colleagues at CERN.\n",
      "*   They created HTTP, which standardized communication between servers and clients.\n",
      "*   On 30 April 1993, CERN put the World Wide Web software in the public domain.\n",
      "\n",
      "I am now handing off to the WriteAgent to write a report on this topic.\n",
      "\n",
      "Output: Okay, I will now write a report on the history of the web based on the research notes.\n",
      "\n",
      "```markdown\n",
      "# A Brief History of the World Wide Web\n",
      "\n",
      "The World Wide Web, often mistakenly used as a synonym for the Internet, is a global information medium that users can access via computers connected to the Internet. It is a service that operates over the Internet, much like email.\n",
      "\n",
      "The development of the World Wide Web was initiated in 1989 by Tim Berners-Lee and his colleagues at CERN, the European Organization for Nuclear Research. Berners-Lee is credited as the inventor of the Web. On March 12, 1989, he proposed what would become the World Wide Web. On August 6, 1991, Berners-Lee published a short summary of the World Wide Web project, inviting collaborators to join the effort.\n",
      "\n",
      "A key element in the Web's development was the creation of the HyperText Transfer Protocol (HTTP). HTTP standardized communication between servers and clients, enabling the seamless transfer of information across the network.\n",
      "\n",
      "In a pivotal move, CERN put the World Wide Web software in the public domain on April 30, 1993. This decision allowed the web to flourish and become the ubiquitous platform it is today.\n",
      "```\n",
      "\n",
      "Now that I have written the report, I will hand it off to the ReviewAgent for feedback.\n",
      "\n",
      "Output: The report provides a good overview of the early history of the World Wide Web, covering its invention, key protocols, and the pivotal decision by CERN to release the software into the public domain. However, it is missing the 21st-century developments as requested in the original prompt.\n",
      "\n",
      "I will request changes to the WriteAgent to include information about the Web 2.0, social media, mobile web, and other significant developments of the 21st century.\n",
      "\n",
      "Output: Okay, I understand. The ReviewAgent has requested that I add information about 21st-century developments to the report. I will now add information about Web 2.0, social media, the rise of mobile web, and other significant advancements since 2000.\n",
      "\n",
      "```markdown\n",
      "# A Brief History of the World Wide Web\n",
      "\n",
      "The World Wide Web, often mistakenly used as a synonym for the Internet, is a global information medium that users can access via computers connected to the Internet. It is a service that operates over the Internet, much like email.\n",
      "\n",
      "The development of the World Wide Web was initiated in 1989 by Tim Berners-Lee and his colleagues at CERN, the European Organization for Nuclear Research. Berners-Lee is credited as the inventor of the Web. On March 12, 1989, he proposed what would become the World Wide Web. On August 6, 1991, Berners-Lee published a short summary of the World Wide Web project, inviting collaborators to join the effort.\n",
      "\n",
      "A key element in the Web's development was the creation of the HyperText Transfer Protocol (HTTP). HTTP standardized communication between servers and clients, enabling the seamless transfer of information across the network.\n",
      "\n",
      "In a pivotal move, CERN put the World Wide Web software in the public domain on April 30, 1993. This decision allowed the web to flourish and become the ubiquitous platform it is today.\n",
      "\n",
      "## 21st Century Developments\n",
      "\n",
      "The 21st century has seen dramatic changes and advancements in the World Wide Web. Key developments include:\n",
      "\n",
      "*   **Web 2.0:** This era marked a shift towards user-generated content, social networking, and interactive web applications. Blogs, wikis, and social media platforms became prominent.\n",
      "*   **Social Media:** Platforms like Facebook, Twitter, YouTube, and Instagram revolutionized communication, information sharing, and social interaction on a global scale.\n",
      "*   **Mobile Web:** The rise of smartphones and mobile devices led to the development of mobile-friendly websites and applications, making the web accessible anytime, anywhere.\n",
      "*   **E-commerce:** Online shopping and digital marketplaces have transformed the retail industry, with companies like Amazon and Alibaba becoming global giants.\n",
      "*   **Cloud Computing:** Cloud-based services and applications have enabled greater scalability, accessibility, and collaboration.\n",
      "*   **The Internet of Things (IoT):** The proliferation of connected devices has expanded the web into the physical world, with everyday objects communicating and sharing data.\n",
      "\n",
      "```\n",
      "\n",
      "Now that I have updated the report, I will hand it off to the ReviewAgent for feedback.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "load_dotenv()\n",
    "tavily_tool = TavilyToolSpec(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "search_web = tavily_tool.to_tool_list()[0]\n",
    "from llama_index.core.agent.workflow import AgentOutput\n",
    "llm = GoogleGenAI(model=\"models/gemini-2.0-flash\", api_key=api_key,temperature = 0)\n",
    "async def record_notes(ctx: Context, notes: str, notes_title: str) -> str:\n",
    "    \"\"\"Useful for recording notes on a given topic.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    if \"research_notes\" not in current_state:\n",
    "        current_state[\"research_notes\"] = {}\n",
    "    current_state[\"research_notes\"][notes_title] = notes\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Notes recorded.\"\n",
    "async def write_report(ctx: Context, report_content: str) -> str:\n",
    "    \"\"\"Useful for writing a report on a given topic.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    current_state[\"report_content\"] = report_content\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Report written.\"\n",
    "\n",
    "\n",
    "async def review_report(ctx: Context, review: str) -> str:\n",
    "    \"\"\"Useful for reviewing a report and providing feedback.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    current_state[\"review\"] = review\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Report reviewed.\"\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "\n",
    "research_agent = FunctionAgent(\n",
    "    name=\"ResearchAgent\",\n",
    "    description=\"Useful for searching the web for information on a given topic and recording notes on the topic.\",\n",
    "    system_prompt=(\n",
    "        \"You are the ResearchAgent that can search the web for information on a given topic and record notes on the topic. \"\n",
    "        \"Once notes are recorded and you are satisfied, you should hand off control to the WriteAgent to write a report on the topic.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[search_web, record_notes],\n",
    "    can_handoff_to=[\"WriteAgent\"],\n",
    ")\n",
    "write_agent = FunctionAgent(\n",
    "    name=\"WriteAgent\",\n",
    "    description=\"Useful for writing a report on a given topic.\",\n",
    "    system_prompt=(\n",
    "        \"You are the WriteAgent that can write a report on a given topic. \"\n",
    "        \"Your report should be in a markdown format. The content should be grounded in the research notes. \"\n",
    "        \"Once the report is written, you should get feedback at least once from the ReviewAgent.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[write_report],\n",
    "    can_handoff_to=[\"ReviewAgent\", \"ResearchAgent\"],\n",
    ")\n",
    "\n",
    "review_agent = FunctionAgent(\n",
    "    name=\"ReviewAgent\",\n",
    "    description=\"Useful for reviewing a report and providing feedback.\",\n",
    "    system_prompt=(\n",
    "        \"You are the ReviewAgent that can review a report and provide feedback. \"\n",
    "        \"Your feedback should either approve the current report or request changes for the WriteAgent to implement.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[review_report],\n",
    "    can_handoff_to=[\"WriteAgent\"],\n",
    ")\n",
    "agent_workflow = AgentWorkflow(\n",
    "    agents=[research_agent, write_agent, review_agent],\n",
    "    root_agent=research_agent.name,\n",
    "    initial_state={\n",
    "        \"research_notes\": {},\n",
    "        \"report_content\": \"Not written yet.\",\n",
    "        \"review\": \"Review required.\",\n",
    "    },\n",
    ")\n",
    "handler = agent_workflow.run(\n",
    "    user_msg=\"\"\"\n",
    "    Write me a report on the history of the web. Briefly describe the history\n",
    "    of the world wide web, including the development of the internet and the\n",
    "    development of the web, including 21st century developments.\n",
    "\"\"\"\n",
    ")\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, AgentOutput):\n",
    "        if event.response.content:\n",
    "            print(\"Output:\", event.response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287ef15",
   "metadata": {},
   "source": [
    "Now, we will be creating a simple Workflow : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c532b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    \n",
    ")\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
    "        # do something here\n",
    "        return StopEvent(result=\"Hello, world!\")\n",
    "\n",
    "\n",
    "w = MyWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f4843",
   "metadata": {},
   "source": [
    "Creating custom events :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01059332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the workflow.\n",
      "First step complete.\n",
      "Second step complete.\n",
      "Workflow complete.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    ")\n",
    "class FirstEvent(Event):\n",
    "    first_output: str\n",
    "\n",
    "\n",
    "class SecondEvent(Event):\n",
    "    second_output: str\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent) -> FirstEvent:\n",
    "        print(ev.first_input)\n",
    "        return FirstEvent(first_output=\"First step complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: FirstEvent) -> SecondEvent:\n",
    "        print(ev.first_output)\n",
    "        return SecondEvent(second_output=\"Second step complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_three(self, ev: SecondEvent) -> StopEvent:\n",
    "        print(ev.second_output)\n",
    "        return StopEvent(result=\"Workflow complete.\")\n",
    "\n",
    "\n",
    "w = MyWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run(first_input=\"Start the workflow.\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a60ffa",
   "metadata": {},
   "source": [
    "Now we will create loops in workflow :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44f68f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's start!!!\n",
      "Bad thing\n",
      "Bad thing\n",
      "Good thing\n",
      "Event over!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    ")\n",
    "import random\n",
    "class LoopEvent(Event):\n",
    "    loop_output: str\n",
    "\n",
    "class Myworkflow(Workflow):\n",
    "    @step\n",
    "    async def step(self,ev : StartEvent | LoopEvent)->StopEvent | LoopEvent:\n",
    "        if isinstance(ev, StartEvent):\n",
    "            print(ev.first_input)\n",
    "        if random.randint(0,1) == 0:\n",
    "            print(\"Bad thing\")\n",
    "            return LoopEvent(loop_output = \"Back to first step\")\n",
    "        else:\n",
    "            print(\"Good thing\")\n",
    "            return StopEvent(result = \"Event over!\")\n",
    "\n",
    "w = Myworkflow(timeout = 20,verbose = False)\n",
    "result = await w.run(first_input = \"Let's start!!!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2bd3ef",
   "metadata": {},
   "source": [
    "Now we will do branching in workflows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca1fc79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to branch A\n",
      "Branch A\n",
      "Branch A\n",
      "Iteration 1: Branch A complete.\n",
      "Go to branch B\n",
      "Branch B\n",
      "Branch B\n",
      "Iteration 2: Branch B complete.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    ")\n",
    "import random\n",
    "class BranchA1Event(Event):\n",
    "    payload: str\n",
    "\n",
    "\n",
    "class BranchA2Event(Event):\n",
    "    payload: str\n",
    "\n",
    "\n",
    "class BranchB1Event(Event):\n",
    "    payload: str\n",
    "\n",
    "\n",
    "class BranchB2Event(Event):\n",
    "    payload: str\n",
    "\n",
    "\n",
    "class BranchWorkflow(Workflow):\n",
    "    @step\n",
    "    async def start(self, ev: StartEvent) -> BranchA1Event | BranchB1Event:\n",
    "        if random.randint(0, 1) == 0:\n",
    "            print(\"Go to branch A\")\n",
    "            return BranchA1Event(payload=\"Branch A\")\n",
    "        else:\n",
    "            print(\"Go to branch B\")\n",
    "            return BranchB1Event(payload=\"Branch B\")\n",
    "\n",
    "    @step\n",
    "    async def step_a1(self, ev: BranchA1Event) -> BranchA2Event:\n",
    "        print(ev.payload)\n",
    "        return BranchA2Event(payload=ev.payload)\n",
    "\n",
    "    @step\n",
    "    async def step_b1(self, ev: BranchB1Event) -> BranchB2Event:\n",
    "        print(ev.payload)\n",
    "        return BranchB2Event(payload=ev.payload)\n",
    "\n",
    "    @step\n",
    "    async def step_a2(self, ev: BranchA2Event) -> StopEvent:\n",
    "        print(ev.payload)\n",
    "        return StopEvent(result=\"Branch A complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_b2(self, ev: BranchB2Event) -> StopEvent:\n",
    "        print(ev.payload)\n",
    "        return StopEvent(result=\"Branch B complete.\")\n",
    "    \n",
    "w = BranchWorkflow(timeout = 20)\n",
    "result = await w.run()\n",
    "print(f\"Iteration 1: {result}\")\n",
    "result = await w.run()\n",
    "print(f\"Iteration 2: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d3d5d",
   "metadata": {},
   "source": [
    "Now we will look at running steps concurrently. Concurrency is when you run a function multiple times but instead of how in parallelism you would run the different instances simultaneously, you would split the resources and you would run each functional call piece wise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "145dfc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query  Query 1\n",
      "Running query  Query 2\n",
      "Running query  Query 3\n",
      "[StepThreeEvent(result='Query 1'), StepThreeEvent(result='Query 3'), StepThreeEvent(result='Query 2')]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context,\n",
    ")\n",
    "import asyncio\n",
    "import random\n",
    "class StepTwoEvent(Event):\n",
    "    query:str\n",
    "class StepThreeEvent(Event):\n",
    "    result :str\n",
    "\n",
    "class ConcurrentFlow(Workflow):\n",
    "    @step\n",
    "    async def start(self, ctx: Context, ev: StartEvent) -> StepTwoEvent:\n",
    "        ctx.send_event(StepTwoEvent(query=\"Query 1\"))\n",
    "        ctx.send_event(StepTwoEvent(query=\"Query 2\"))\n",
    "        ctx.send_event(StepTwoEvent(query=\"Query 3\"))\n",
    "\n",
    "    @step(num_workers=4)\n",
    "    async def step_two(self, ctx: Context, ev: StepTwoEvent) -> StepThreeEvent:\n",
    "        print(\"Running query \", ev.query)\n",
    "        await asyncio.sleep(random.randint(1, 5))\n",
    "        return StepThreeEvent(result=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def step_three(self, ctx: Context, ev: StepThreeEvent) -> StopEvent:\n",
    "        # wait until we receive 3 events\n",
    "        result = ctx.collect_events(ev, [StepThreeEvent] * 3)\n",
    "        if result is None:\n",
    "            return None\n",
    "\n",
    "        # do something with all 3 results together\n",
    "        print(result)\n",
    "        return StopEvent(result=\"Done\")\n",
    "c = ConcurrentFlow(timeout = 20)\n",
    "result = await c.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d04339",
   "metadata": {},
   "source": [
    "Now, we will create sub workflows so that lets say your model involves two steps pre processing and then based on that sending a text you can use a main workflow and use that as a parent class for the sub workflow :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09218a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up\n",
      "Sending an email\n",
      "Also sending a text message\n",
      "Finishing up\n",
      "Initial query\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context,\n",
    ")\n",
    "\n",
    "\n",
    "class Step2Event(Event):\n",
    "    query: str\n",
    "\n",
    "\n",
    "class Step3Event(Event):\n",
    "    query: str\n",
    "\n",
    "\n",
    "class MainWorkflow(Workflow):\n",
    "    @step\n",
    "    async def start(self, ev: StartEvent) -> Step2Event:\n",
    "        print(\"Starting up\")\n",
    "        return Step2Event(query=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: Step2Event) -> Step3Event:\n",
    "        print(\"Sending an email\")\n",
    "        return Step3Event(query=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def step_three(self, ev: Step3Event) -> StopEvent:\n",
    "        print(\"Finishing up\")\n",
    "        return StopEvent(result=ev.query)\n",
    "\n",
    "class Step2BEvent(Event):\n",
    "    query: str\n",
    "\n",
    "\n",
    "class CustomWorkflow(MainWorkflow):\n",
    "    @step\n",
    "    async def step_two(self, ev: Step2Event) -> Step2BEvent:\n",
    "        print(\"Sending an email\")\n",
    "        return Step2BEvent(query=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def step_two_b(self, ev: Step2BEvent) -> Step3Event:\n",
    "        print(\"Also sending a text message\")\n",
    "        return Step3Event(query=ev.query)\n",
    "w = CustomWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run(query=\"Initial query\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
